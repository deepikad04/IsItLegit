from typing import Annotated
from uuid import UUID

from fastapi import APIRouter, Depends, File, HTTPException, Query, Request, UploadFile, status
from sqlalchemy.orm import Session
from sqlalchemy.orm.attributes import flag_modified

from database import get_db
from models.user import User
from models.simulation import Simulation
from models.decision import Decision
from routers.auth import get_current_user, limiter
from models.behavior_profile import BehaviorProfile
from services.gemini_service import GeminiService
from schemas.reflection import (
    ReflectionResponse,
    Counterfactual,
    WhyThisDecisionResponse,
    ProComparisonResponse,
)

router = APIRouter(prefix="/api/reflection", tags=["reflection"])


@router.get("/{simulation_id}", response_model=ReflectionResponse, summary="Get reflection analysis")
@limiter.limit("10/minute")
async def get_reflection(
    request: Request,
    simulation_id: UUID,
    current_user: Annotated[User, Depends(get_current_user)],
    db: Session = Depends(get_db)
):
    """Get full reflection analysis for a completed simulation."""
    simulation = db.query(Simulation).filter(
        Simulation.id == simulation_id,
        Simulation.user_id == current_user.id
    ).first()

    if not simulation:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Simulation not found"
        )

    if simulation.status != "completed":
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Simulation must be completed to get reflection"
        )

    # Check if analysis already exists
    if simulation.gemini_analysis:
        return ReflectionResponse(**simulation.gemini_analysis)

    # Get decisions for analysis
    decisions = db.query(Decision).filter(
        Decision.simulation_id == simulation_id
    ).order_by(Decision.simulation_time).all()

    # Generate analysis with Gemini service
    gemini = GeminiService()
    analysis = await gemini.analyze_simulation(
        simulation=simulation,
        decisions=decisions,
        scenario=simulation.scenario
    )

    # Store analysis (mode='json' ensures UUIDs are serialized as strings)
    analysis_data = analysis.model_dump(mode='json')
    # Store AI metadata alongside analysis for the /ai-metadata endpoint
    analysis_data["_source"] = gemini.current_source
    if gemini._last_thinking:
        analysis_data["_thinking"] = gemini._last_thinking
    simulation.gemini_analysis = analysis_data
    db.commit()

    return analysis


@router.get("/{simulation_id}/counterfactuals", response_model=list[Counterfactual], summary="Get counterfactual timelines")
@limiter.limit("10/minute")
async def get_counterfactuals(
    request: Request,
    simulation_id: UUID,
    current_user: Annotated[User, Depends(get_current_user)],
    db: Session = Depends(get_db)
):
    """Get counterfactual alternate timelines for a simulation."""
    simulation = db.query(Simulation).filter(
        Simulation.id == simulation_id,
        Simulation.user_id == current_user.id
    ).first()

    if not simulation:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Simulation not found"
        )

    if simulation.status != "completed":
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Simulation must be completed"
        )

    # Check if counterfactuals already exist
    if simulation.counterfactuals:
        return [Counterfactual(**cf) for cf in simulation.counterfactuals]

    # Get decisions
    decisions = db.query(Decision).filter(
        Decision.simulation_id == simulation_id
    ).order_by(Decision.simulation_time).all()

    # Generate counterfactuals
    gemini = GeminiService()
    counterfactuals = await gemini.generate_counterfactuals(
        simulation=simulation,
        decisions=decisions,
        scenario=simulation.scenario
    )

    # Store
    simulation.counterfactuals = [cf.model_dump(mode='json') for cf in counterfactuals]
    db.commit()

    return counterfactuals


@router.get("/{simulation_id}/ai-metadata", summary="AI source & thinking metadata")
async def get_ai_metadata(
    simulation_id: UUID,
    current_user: Annotated[User, Depends(get_current_user)],
    db: Session = Depends(get_db),
):
    """Returns whether the analysis was generated by Gemini or heuristic fallback,
    plus Gemini's thinking/reasoning text if available."""
    simulation = db.query(Simulation).filter(
        Simulation.id == simulation_id,
        Simulation.user_id == current_user.id,
    ).first()

    if not simulation:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Simulation not found")

    analysis = simulation.gemini_analysis or {}
    cache = simulation.gemini_cache or {}

    # Collect source + thinking from all stored results
    source = analysis.get("_source") or "unknown"
    thinking = analysis.get("_thinking")

    # Also check gemini_cache entries for thinking from other call types
    thinking_sections = {}
    if thinking:
        thinking_sections["reflection"] = thinking
    for key in ("why", "pro_comparison", "coaching", "bias_heatmap", "rationale_review",
                "bias_classifier", "confidence_calibration"):
        entry = cache.get(key, {})
        if isinstance(entry, dict) and entry.get("_thinking"):
            thinking_sections[key] = entry["_thinking"]

    # Current GeminiService state
    gemini = GeminiService()
    current_mode = gemini.current_source

    return {
        "source": source,
        "current_mode": current_mode,
        "thinking": thinking_sections if thinking_sections else None,
        "model": gemini.model_name,
    }


@router.get("/{simulation_id}/quick-summary")
async def get_quick_summary(
    simulation_id: UUID,
    current_user: Annotated[User, Depends(get_current_user)],
    db: Session = Depends(get_db)
):
    """Get a quick summary without full Gemini analysis."""
    simulation = db.query(Simulation).filter(
        Simulation.id == simulation_id,
        Simulation.user_id == current_user.id
    ).first()

    if not simulation:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Simulation not found"
        )

    if simulation.status != "completed":
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Simulation must be completed"
        )

    decisions = db.query(Decision).filter(
        Decision.simulation_id == simulation_id
    ).all()

    outcome = simulation.final_outcome or {}

    return {
        "simulation_id": simulation.id,
        "outcome": outcome.get("profit_loss", 0),
        "outcome_percent": outcome.get("profit_loss_percent", 0),
        "process_quality_score": simulation.process_quality_score,
        "total_decisions": len(decisions),
        "time_taken": simulation.current_time_elapsed,
        "scenario_name": simulation.scenario.name
    }


# ── Helper: persist to gemini_cache ─────────────────────────────────

def _get_cached(simulation, key):
    """Return cached Gemini result from DB if available."""
    cache = simulation.gemini_cache or {}
    return cache.get(key)


def _set_cached(simulation, key, data, db, gemini: GeminiService | None = None):
    """Persist a Gemini result to the gemini_cache JSONB column."""
    import json

    # Ensure data is JSON-serializable (handles UUID, datetime, etc.)
    safe_data = json.loads(json.dumps(data, default=str))

    # Tag with AI source and thinking metadata
    if gemini:
        safe_data["_source"] = gemini.current_source
        if gemini._last_thinking:
            safe_data["_thinking"] = gemini._last_thinking

    if not simulation.gemini_cache:
        simulation.gemini_cache = {}
    simulation.gemini_cache[key] = safe_data
    flag_modified(simulation, "gemini_cache")
    db.commit()


# ── Helper to load simulation + decisions with auth check ─────────────

async def _load_completed_simulation(
    simulation_id: UUID,
    current_user: User,
    db: Session,
):
    """Shared helper: load simulation, check ownership and completion."""
    simulation = db.query(Simulation).filter(
        Simulation.id == simulation_id,
        Simulation.user_id == current_user.id
    ).first()

    if not simulation:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Simulation not found")
    if simulation.status != "completed":
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Simulation must be completed")

    decisions = db.query(Decision).filter(
        Decision.simulation_id == simulation_id
    ).order_by(Decision.simulation_time).all()

    return simulation, decisions


@router.get("/{simulation_id}/why", response_model=WhyThisDecisionResponse, summary="Why this decision?")
@limiter.limit("10/minute")
async def why_this_decision(
    request: Request,
    simulation_id: UUID,
    current_user: Annotated[User, Depends(get_current_user)],
    db: Session = Depends(get_db)
):
    """'Why this decision?' — Gemini explains detected biases using the user's actual actions."""
    simulation, decisions = await _load_completed_simulation(simulation_id, current_user, db)

    # Check DB cache first
    cached = _get_cached(simulation, "why")
    if cached:
        return WhyThisDecisionResponse(**cached)

    gemini = GeminiService()
    result = await gemini.explain_decisions(
        simulation=simulation,
        decisions=decisions,
        scenario=simulation.scenario,
    )

    _set_cached(simulation, "why", result.model_dump(mode='json'), db, gemini)
    return result


@router.get("/{simulation_id}/pro-comparison", summary="Expert vs your decisions")
@limiter.limit("10/minute")
async def pro_comparison(
    request: Request,
    simulation_id: UUID,
    current_user: Annotated[User, Depends(get_current_user)],
    db: Session = Depends(get_db)
):
    """'What would a pro do?' — side-by-side comparison with an expert decision path."""
    simulation, decisions = await _load_completed_simulation(simulation_id, current_user, db)

    # Check DB cache first
    cached = _get_cached(simulation, "pro_comparison")
    if cached:
        return cached

    gemini = GeminiService()
    result = await gemini.compare_with_pro(
        simulation=simulation,
        decisions=decisions,
        scenario=simulation.scenario,
    )

    # Attach price history + algorithmic pro trader baseline
    from services.simulation_engine import SimulationEngine
    engine = SimulationEngine(simulation.scenario)
    time_limit = simulation.scenario.time_pressure_seconds
    price_history = [
        {"time": t, "price": round(engine.price_timeline.get(t, 0), 2)}
        for t in range(0, time_limit + 1, max(1, time_limit // 60))
    ]

    # Run algorithmic trader on same timeline
    algo_result = engine.run_algorithmic_trader(decisions)

    data = result.model_dump(mode='json')
    data["price_history"] = price_history
    data["algorithmic_baseline"] = algo_result

    _set_cached(simulation, "pro_comparison", data, db, gemini)
    return data


@router.get("/{simulation_id}/coaching", summary="Personalized coaching & profile update")
@limiter.limit("10/minute")
async def get_coaching(
    request: Request,
    simulation_id: UUID,
    current_user: Annotated[User, Depends(get_current_user)],
    db: Session = Depends(get_db)
):
    """Personalized coaching that adapts based on the user's behavior profile history."""
    simulation, decisions = await _load_completed_simulation(simulation_id, current_user, db)

    # Check DB cache first
    cached = _get_cached(simulation, "coaching")
    if cached:
        return cached

    # Load behavior profile for personalization
    profile = db.query(BehaviorProfile).filter(
        BehaviorProfile.user_id == current_user.id
    ).first()

    profile_data = profile.profile_data if profile else None

    gemini = GeminiService()
    persona, _ = gemini._determine_persona(profile_data)
    coaching_message = await gemini.generate_coaching(
        simulation=simulation,
        decisions=decisions,
        scenario=simulation.scenario,
        behavior_profile=profile_data,
    )

    # Also update behavior profile with data from this simulation
    updated_profile = await gemini.update_behavior_profile(
        user_id=str(current_user.id),
        simulation=simulation,
        decisions=decisions,
        scenario=simulation.scenario,
        existing_profile=profile_data,
    )

    # Persist updated profile
    if profile:
        profile.profile_data = updated_profile
        profile.total_simulations_analyzed += 1
    else:
        from datetime import datetime
        profile = BehaviorProfile(
            user_id=current_user.id,
            profile_data=updated_profile,
            total_simulations_analyzed=1,
            last_updated=datetime.utcnow(),
        )
        db.add(profile)

    result = {
        "coaching_message": coaching_message,
        "persona": persona,
        "profile_updated": True,
    }

    _set_cached(simulation, "coaching", result, db, gemini)
    return result


@router.get("/{simulation_id}/full", summary="Full reflection + counterfactuals (batch)")
@limiter.limit("5/minute")
async def get_full_reflection(
    request: Request,
    simulation_id: UUID,
    current_user: Annotated[User, Depends(get_current_user)],
    db: Session = Depends(get_db),
):
    """Get reflection + counterfactuals + coaching in one shot (single Gemini call)."""
    simulation, decisions = await _load_completed_simulation(simulation_id, current_user, db)

    # Return cached if available
    if simulation.gemini_analysis and simulation.counterfactuals:
        return {
            "reflection": simulation.gemini_analysis,
            "counterfactuals": simulation.counterfactuals,
            "coaching_message": simulation.gemini_analysis.get("coaching_message"),
            "persona": simulation.gemini_analysis.get("persona", "supportive"),
        }

    profile = db.query(BehaviorProfile).filter(
        BehaviorProfile.user_id == current_user.id
    ).first()

    gemini = GeminiService()
    result = await gemini.batch_analyze(
        simulation=simulation,
        decisions=decisions,
        scenario=simulation.scenario,
        behavior_profile=profile.profile_data if profile else None,
    )

    # Persist — tag with AI source and thinking metadata
    analysis_data = result["reflection"]
    if isinstance(analysis_data, dict):
        analysis_data["_source"] = gemini.current_source
        if gemini._last_thinking:
            analysis_data["_thinking"] = gemini._last_thinking
    simulation.gemini_analysis = analysis_data
    simulation.counterfactuals = result["counterfactuals"]
    db.commit()

    return result


@router.get("/{simulation_id}/bias-heatmap", summary="Bias intensity heatmap over time")
@limiter.limit("10/minute")
async def get_bias_heatmap(
    request: Request,
    simulation_id: UUID,
    current_user: Annotated[User, Depends(get_current_user)],
    db: Session = Depends(get_db),
):
    """Bias intensity timeline across all decision points."""
    simulation, decisions = await _load_completed_simulation(simulation_id, current_user, db)

    # Check DB cache first
    cached = _get_cached(simulation, "bias_heatmap")
    if cached:
        return cached

    gemini = GeminiService()
    result = await gemini.analyze_bias_timeline(
        simulation=simulation,
        decisions=decisions,
        scenario=simulation.scenario,
    )

    # result could be a Pydantic model or dict
    data = result.model_dump(mode='json') if hasattr(result, 'model_dump') else result
    _set_cached(simulation, "bias_heatmap", data, db, gemini)
    return result


@router.get("/{simulation_id}/rationale-review", summary="AI critique of your stated reasoning")
@limiter.limit("10/minute")
async def review_rationales(
    request: Request,
    simulation_id: UUID,
    current_user: Annotated[User, Depends(get_current_user)],
    db: Session = Depends(get_db),
):
    """Gemini critiques the user's stated rationales for each decision."""
    simulation, decisions = await _load_completed_simulation(simulation_id, current_user, db)

    decisions_with_rationale = [d for d in decisions if d.rationale]
    if not decisions_with_rationale:
        return {"reviews": [], "summary": "No rationales were provided during this simulation.", "overall_reasoning_quality": 3}

    # Check DB cache first
    cached = _get_cached(simulation, "rationale_review")
    if cached:
        return cached

    gemini = GeminiService()
    result = await gemini.review_rationales(
        simulation=simulation,
        decisions=decisions_with_rationale,
        scenario=simulation.scenario,
    )

    data = result.model_dump(mode='json') if hasattr(result, 'model_dump') else result
    _set_cached(simulation, "rationale_review", data, db, gemini)
    return result


@router.get("/{simulation_id}/calibration", summary="Confidence calibration report")
@limiter.limit("10/minute")
async def get_calibration(
    request: Request,
    simulation_id: UUID,
    current_user: Annotated[User, Depends(get_current_user)],
    db: Session = Depends(get_db),
):
    """Confidence calibration score — did confidence match outcomes?"""
    simulation, decisions = await _load_completed_simulation(simulation_id, current_user, db)

    from services.simulation_engine import SimulationEngine
    engine = SimulationEngine(simulation.scenario)
    return engine.get_calibration_report(decisions)


@router.get("/{simulation_id}/outcome-distribution", summary="Monte Carlo outcome distribution")
@limiter.limit("5/minute")
async def get_outcome_distribution(
    request: Request,
    simulation_id: UUID,
    current_user: Annotated[User, Depends(get_current_user)],
    db: Session = Depends(get_db),
):
    """Monte Carlo outcome distribution — same decisions, 100 different markets."""
    simulation, decisions = await _load_completed_simulation(simulation_id, current_user, db)

    from services.simulation_engine import SimulationEngine
    engine = SimulationEngine(simulation.scenario)
    return engine.monte_carlo_outcomes(decisions, n=100)


@router.get("/{simulation_id}/bias-classifier", summary="AI-powered bias classification")
@limiter.limit("10/minute")
async def get_bias_classification(
    request: Request,
    simulation_id: UUID,
    current_user: Annotated[User, Depends(get_current_user)],
    db: Session = Depends(get_db),
):
    """Classify biases using Gemini AI analysis of decision features + behavioral trace."""
    simulation, decisions = await _load_completed_simulation(simulation_id, current_user, db)

    # Check DB cache first
    cached = _get_cached(simulation, "bias_classifier")
    if cached:
        return cached

    from services.simulation_engine import SimulationEngine
    from services.bias_classifier import extract_decision_features

    engine = SimulationEngine(simulation.scenario)

    # Extract numerical features for each decision (fed to Gemini alongside the trace)
    decision_features = []
    for i, d in enumerate(decisions):
        features = extract_decision_features(
            d, engine.price_timeline, engine.time_limit,
            engine.initial_price, engine.initial_balance, decisions, i,
        )
        decision_features.append(features)

    # Gemini-first: AI analyzes both qualitative trace + quantitative features
    gemini = GeminiService()
    result = await gemini.classify_biases_with_gemini(
        simulation=simulation,
        decisions=decisions,
        scenario=simulation.scenario,
        decision_features=decision_features,
    )

    _set_cached(simulation, "bias_classifier", result, db, gemini)
    return result


@router.get("/{simulation_id}/confidence-calibration", summary="AI self-evaluation of pattern confidence")
@limiter.limit("10/minute")
async def get_confidence_calibration(
    request: Request,
    simulation_id: UUID,
    current_user: Annotated[User, Depends(get_current_user)],
    db: Session = Depends(get_db),
):
    """Gemini self-evaluates its pattern detections against observable evidence."""
    simulation, decisions = await _load_completed_simulation(simulation_id, current_user, db)

    # Check DB cache first
    cached = _get_cached(simulation, "confidence_calibration")
    if cached:
        return cached

    # Get the reflection analysis (must exist to have patterns)
    if not simulation.gemini_analysis:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Run the reflection analysis first to get pattern detections",
        )

    patterns = simulation.gemini_analysis.get("patterns_detected", [])
    if not patterns:
        result = {
            "calibrated_patterns": [],
            "overall_evidence_quality": "weak",
            "abstained_patterns": [],
            "calibration_summary": "No patterns were detected in the reflection analysis.",
        }
        _set_cached(simulation, "confidence_calibration", result, db)
        return result

    # Gather observable evidence signals independently
    from services.simulation_engine import SimulationEngine
    from services.confidence_calibrator import _gather_evidence_signals

    engine = SimulationEngine(simulation.scenario)
    evidence_signals = _gather_evidence_signals(
        patterns=patterns,
        decisions=decisions,
        price_timeline=engine.price_timeline,
        events=engine.events,
        time_limit=engine.time_limit,
        initial_price=engine.initial_price,
        initial_balance=engine.initial_balance,
    )

    # Gemini-first: AI self-evaluates its own detections against evidence
    gemini = GeminiService()
    result = await gemini.calibrate_pattern_confidence(
        simulation=simulation,
        decisions=decisions,
        scenario=simulation.scenario,
        patterns_detected=patterns,
        evidence_signals=evidence_signals,
    )

    _set_cached(simulation, "confidence_calibration", result, db, gemini)
    return result


@router.get("/{simulation_id}/counterfactual-isolation", summary="Isolate impact of a single decision")
@limiter.limit("5/minute")
async def counterfactual_isolation(
    request: Request,
    simulation_id: UUID,
    current_user: Annotated[User, Depends(get_current_user)],
    decision_index: int = Query(..., ge=0, description="Which decision to isolate"),
    db: Session = Depends(get_db),
):
    """Show causal impact of changing a single decision."""
    simulation, decisions = await _load_completed_simulation(simulation_id, current_user, db)

    if decision_index >= len(decisions):
        raise HTTPException(status_code=400, detail="Invalid decision index")

    # Check DB cache (keyed by decision index)
    cache_key = f"counterfactual_isolation_{decision_index}"
    cached = _get_cached(simulation, cache_key)
    if cached:
        return cached

    gemini = GeminiService()
    result = await gemini.isolate_counterfactual(
        simulation=simulation,
        decisions=decisions,
        scenario=simulation.scenario,
        target_decision_index=decision_index,
    )

    data = result.model_dump(mode='json') if hasattr(result, 'model_dump') else result
    _set_cached(simulation, cache_key, data, db, gemini)
    return result


# ── Chart Analysis (Multimodal Vision) ─────────────────────────────

ALLOWED_IMAGE_TYPES = {"image/png", "image/jpeg", "image/webp", "image/gif"}
MAX_IMAGE_SIZE = 10 * 1024 * 1024  # 10 MB


@router.post("/chart-analysis", summary="Analyze a trading chart image with Gemini Vision")
@limiter.limit("5/minute")
async def analyze_chart(
    request: Request,
    current_user: Annotated[User, Depends(get_current_user)],
    file: UploadFile = File(...),
):
    """Upload a trading chart screenshot for AI-powered visual analysis.
    Gemini Vision identifies patterns, trends, and cognitive bias risks."""
    if file.content_type not in ALLOWED_IMAGE_TYPES:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Unsupported image type: {file.content_type}. Use PNG, JPEG, WebP, or GIF.",
        )

    image_bytes = await file.read()
    if len(image_bytes) > MAX_IMAGE_SIZE:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Image too large. Maximum size is 10 MB.",
        )

    gemini = GeminiService()
    result = await gemini.analyze_chart(image_bytes, file.content_type)
    return result
